{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIFTandSURF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVab3B4FoUa6",
        "outputId": "f5c93ae5-b075-4323-fa16-e98ffadef783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: http://pypi.douban.com/simple\n",
            "Requirement already satisfied: opencv-contrib-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==3.4.2.17) (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install -i http://pypi.douban.com/simple --trusted-host pypi.douban.com opencv-contrib-python==3.4.2.17\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Extraction: de=0 then SURF\n",
        "#Feature Extraction: de any other value then SIFT\n",
        "#KMeans:cannot use huge number of descriptors\n",
        "\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "from pathlib import Path\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import hog\n",
        "from skimage import exposure\n",
        "def makedataset(de,km):\n",
        "  noi = 0\n",
        "  dl=[]\n",
        "  kpl=[]\n",
        "  data = []\n",
        "  label=[]\n",
        "  tdata=[]\n",
        "  tlabel=[]\n",
        "  rawImg =[]\n",
        "  for t in range(1,11):\n",
        "    k=t\n",
        "    if(t==10):\n",
        "      t=\"00010\"+\"/train.txt\"\n",
        "      fol=\"00010\"\n",
        "      m=\"00010\"+\"/test.txt\"\n",
        "    else:\n",
        "      fol=\"0000\"+str(t)\n",
        "      t=\"0000\"+str(t)+\"/train.txt\"\n",
        "      m=\"0000\"+str(k)+\"/test.txt\"\n",
        "      \n",
        "    \n",
        "    fpath = os.path.join(\"/content/drive/MyDrive/CS893 Sp2022 A1 Dataset/\",t)\n",
        "    tpath = os.path.join(\"/content/drive/MyDrive/CS893 Sp2022 A1 Dataset/\",m)\n",
        "    f = open(fpath, \"r\")\n",
        "    te= open(tpath,\"r\")\n",
        "    if(de==0):\n",
        "      sift = cv2.xfeatures2d.SURF_create(300)\n",
        "    else:\n",
        "      sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "\n",
        "\n",
        "    for x in f:\n",
        "      path = os.path.join(\"/content/drive/MyDrive/CS893 Sp2022 A1 Dataset/\",fol.strip(),x.strip())\n",
        "      img = cv2.imread(path)\n",
        "      gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "      kp, des = sift.detectAndCompute(gray_img, None)\n",
        "      \n",
        "      dl.append(des)\n",
        "      kpl.append(kp)\n",
        "      if len(kp)!=0:\n",
        "        for d in des:\n",
        "          data.append(d)        \n",
        "      noi=noi+1\n",
        "      label.append(k)\n",
        "    f.close()\n",
        "    for x in te:\n",
        "      path = os.path.join(\"/content/drive/MyDrive/CS893 Sp2022 A1 Dataset/\",fol.strip(),x.strip())\n",
        "      img = cv2.imread(path)\n",
        "      tdata.append(img)\n",
        "      tlabel.append(k)\n",
        "  batch_size = 200\n",
        "  kmeans = MiniBatchKMeans(n_clusters=km, batch_size=batch_size, verbose=1).fit(data)\n",
        "\n",
        "  return dl,label,tdata,tlabel,sift,data,kpl,kmeans\n"
      ],
      "metadata": {
        "id": "UEgB3Y9oorwh"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creation of histogram which will be used as feature for training dataset\n",
        "import numpy as np\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "def featureCraetion(dL,k):\n",
        "   kmeans.verbose = False\n",
        "   histo_list = []\n",
        "   for des in dl:\n",
        "      histo = np.zeros(k)\n",
        "      nkp = np.size(kpl)\n",
        "      if type(des) != type(None):\n",
        "        for d in des:\n",
        "          idx = kmeans.predict([d])\n",
        "          histo[idx] += 1/nkp # Because we need normalized histograms, I prefere to add 1/nkp directly\n",
        "      histo_list.append(histo)\n",
        "   return histo_list"
      ],
      "metadata": {
        "id": "pmTp-kMQqy8G"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creation of histogram which will be used as feature for testing dataset\n",
        "def creaturetrainingfeature(td,k):\n",
        "  X_List=[]\n",
        "  for d in td:\n",
        "    kp, des = sift.detectAndCompute(d, None)\n",
        "    x = np.zeros(k)\n",
        "    nkp = np.size(kp)\n",
        "    if len(kp)!=0:\n",
        "      for d in des:\n",
        "        idx = kmeans.predict([d])\n",
        "        x[idx] += 1/nkp\n",
        "    X_List.append(x)\n",
        "  return X_List\n"
      ],
      "metadata": {
        "id": "XNTSzWGguu6P"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "def classification(histo_list,label,X_List,tlabel):\n",
        "  #Create a Gaussian Classifier\n",
        "  clf=RandomForestClassifier(n_estimators=130)\n",
        "\n",
        "  #Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "  clf.fit(histo_list, label)\n",
        "\n",
        "  y_pred=clf.predict(X_List)\n",
        "  cm = confusion_matrix(tlabel, y_pred)\n",
        "  print(\"Classifiaction Report using RandomForest\")\n",
        "  print(metrics.classification_report(tlabel, y_pred, digits=3))\n",
        "  gnb = GaussianNB().fit(histo_list, label)\n",
        "  gnb_predictions = gnb.predict(X_List)\n",
        " \n",
        "  cm = confusion_matrix(tlabel, gnb_predictions)\n",
        "  t=metrics.classification_report(tlabel, gnb_predictions, digits=2)\n",
        "  print(t)"
      ],
      "metadata": {
        "id": "3_2owPSNrI-Y"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SURF Feature extraction\n",
        "[dl,label,tdata,tlabel,sift,data,kpl,kmeans]= makedataset(0,900)\n",
        "histo_list=featureCraetion(dl,900)\n",
        "X_List = creaturetrainingfeature(tdata,900)\n"
      ],
      "metadata": {
        "id": "-75EFRixwBSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SURF Feature Extraction\")\n",
        "classification(histo_list,label,X_List,tlabel)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l32f_Mm5rlkL",
        "outputId": "8ceedf94-93d1-45cd-d7a5-a6acf4c00f16"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SURF Feature Extraction\n",
            "Classifiaction Report using RandomForest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1      0.944     0.850     0.895        20\n",
            "           2      0.952     1.000     0.976        20\n",
            "           3      0.947     0.900     0.923        20\n",
            "           4      1.000     1.000     1.000        20\n",
            "           5      0.952     1.000     0.976        20\n",
            "           6      0.870     1.000     0.930        20\n",
            "           7      0.750     0.900     0.818        20\n",
            "           8      1.000     0.700     0.824        20\n",
            "           9      0.909     1.000     0.952        20\n",
            "          10      0.889     0.800     0.842        20\n",
            "\n",
            "    accuracy                          0.915       200\n",
            "   macro avg      0.921     0.915     0.914       200\n",
            "weighted avg      0.921     0.915     0.914       200\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.58      0.55      0.56        20\n",
            "           2       0.80      0.40      0.53        20\n",
            "           3       0.45      0.90      0.60        20\n",
            "           4       0.75      0.90      0.82        20\n",
            "           5       0.93      0.65      0.76        20\n",
            "           6       1.00      0.55      0.71        20\n",
            "           7       1.00      0.15      0.26        20\n",
            "           8       0.26      0.70      0.38        20\n",
            "           9       0.72      0.65      0.68        20\n",
            "          10       1.00      0.35      0.52        20\n",
            "\n",
            "    accuracy                           0.58       200\n",
            "   macro avg       0.75      0.58      0.58       200\n",
            "weighted avg       0.75      0.58      0.58       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[dl,label,tdata,tlabel,sift,data,kpl,kmeans]= makedataset(1,2000)\n",
        "histo_list=featureCraetion(dl,2000)\n",
        "X_List = creaturetrainingfeature(tdata,2000)\n",
        "\n"
      ],
      "metadata": {
        "id": "TMOaWqIz1BEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SIFT Feauture Extraction With Classification\")\n",
        "classification(histo_list,label,X_List,tlabel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSSXl20p2rsM",
        "outputId": "cae4b036-eb63-4455-fb48-360f94d07703"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SIFT Feauture Extraction With Classification\n",
            "Classifiaction Report using RandomForest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1      0.947     0.900     0.923        20\n",
            "           2      1.000     1.000     1.000        20\n",
            "           3      1.000     0.950     0.974        20\n",
            "           4      1.000     1.000     1.000        20\n",
            "           5      0.714     0.750     0.732        20\n",
            "           6      0.889     0.800     0.842        20\n",
            "           7      0.667     0.600     0.632        20\n",
            "           8      0.947     0.900     0.923        20\n",
            "           9      0.952     1.000     0.976        20\n",
            "          10      0.640     0.800     0.711        20\n",
            "\n",
            "    accuracy                          0.870       200\n",
            "   macro avg      0.876     0.870     0.871       200\n",
            "weighted avg      0.876     0.870     0.871       200\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.67      0.70      0.68        20\n",
            "           2       1.00      0.90      0.95        20\n",
            "           3       0.48      1.00      0.65        20\n",
            "           4       0.74      1.00      0.85        20\n",
            "           5       0.62      0.40      0.48        20\n",
            "           6       1.00      0.25      0.40        20\n",
            "           7       0.67      0.20      0.31        20\n",
            "           8       0.56      1.00      0.71        20\n",
            "           9       0.95      0.95      0.95        20\n",
            "          10       0.83      0.50      0.62        20\n",
            "\n",
            "    accuracy                           0.69       200\n",
            "   macro avg       0.75      0.69      0.66       200\n",
            "weighted avg       0.75      0.69      0.66       200\n",
            "\n"
          ]
        }
      ]
    }
  ]
}